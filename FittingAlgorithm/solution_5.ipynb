{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save and restore models for additional training\n",
    "# import libraries\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import preprocessing\n",
    "import sys\n",
    "import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('ggplot') # use this plot style\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(array):\n",
    "    u = array.mean()\n",
    "    s = array.std()\n",
    "    norm = (array - u) / s\n",
    "    return u, s, norm\n",
    "\n",
    "def min_max(array, min=0, max=1):\n",
    "    X_std = (array - array.min(axis=0)) / (array.max(axis=0) - array.min(axis=0))\n",
    "    X_scaled = X_std * (max - min) + min\n",
    "    return X_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import gaussian_kde\n",
    "\n",
    "N = 10000\n",
    "def generate_original_data(size=N):\n",
    "    Wafer1 = np.random.normal(loc = 0.0, scale = 1.0, size = N)\n",
    "    Wafer2 = np.random.normal(loc = 2.0, scale = 1.0, size = N)\n",
    "    pdf1 = gaussian_kde(Wafer1)\n",
    "    pdf2 = gaussian_kde(Wafer2)\n",
    "    pdf1.set_bandwidth(bw_method=pdf1.factor / 3.)\n",
    "    pdf2.set_bandwidth(bw_method=pdf2.factor / 3.)\n",
    "    return Wafer1,Wafer2,pdf1, pdf2\n",
    "\n",
    "# Wafer1,Wafer2,pdf1,pdf2 = generate_original_data(size=N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_experiment_data(size=N):\n",
    "    Wafer1,Wafer2,pdf1,pdf2 = generate_original_data(size=N)\n",
    "    X1 = np.linspace(min(Wafer1), max(Wafer1), num=N)\n",
    "    X_pool = pdf1(X1)\n",
    "    X_pool = np.reshape(X_pool, (N,1))\n",
    "    X2 = np.linspace(min(Wafer2), max(Wafer2), num=N)\n",
    "    Y_pool = pdf2(X2)\n",
    "    Y_pool = np.reshape(Y_pool, (N,1))\n",
    "\n",
    "    # sample size of 15%\n",
    "    sample = int(N * 0.15)\n",
    "    # 15% test\n",
    "    test_x = X_pool[0:sample]\n",
    "    # 15% validation\n",
    "    valid_x = X_pool[sample:sample*2]\n",
    "    # 70% training\n",
    "    train_x = X_pool[sample*2:]\n",
    "    print('Testing data points: ' + str(test_x.shape))\n",
    "    print('Validation data points: ' + str(valid_x.shape), type(valid_x))\n",
    "    print('Training data points: ' + str(train_x.shape))\n",
    "    # Let's compute the ouput using 2 for a and 2 for b\n",
    "    test_y = Y_pool[0:sample]\n",
    "    valid_y = Y_pool[sample:sample*2]\n",
    "    train_y = Y_pool[sample*2:]\n",
    "\n",
    "    # scale x and y (I choose to only scale y since x seemed already to be close enough to min=0, max=1)\n",
    "    #test_x = min_max(test_x)\n",
    "    test_y = min_max(test_y)\n",
    "    #valid_x = min_max(valid_x)\n",
    "    valid_y = min_max(valid_y)\n",
    "    #train_x = min_max(train_x)\n",
    "    train_y = min_max(train_y)\n",
    "    # Normalize x and y (I choose to only normalize y since x seemed already to be close enough to mean=0, std=1)\n",
    "    #u_test_x, s_test_x, test_x = normalize(test_x)\n",
    "    u_test_y, s_test_y, test_y = normalize(test_y)\n",
    "    #u_valid_x, s_valid_x, valid_x = normalize(valid_x)\n",
    "    u_valid_y, s_valid_y, valid_y = normalize(valid_y)\n",
    "    #u_train_x, s_train_x, train_x = normalize(train_x)\n",
    "    u_train_y, s_train_y, train_y = normalize(train_y)\n",
    "    \n",
    "    return train_x, train_y,valid_x,valid_y,test_x,test_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper Function\n",
    "def add_layer(inputs, in_size, out_size, activation_function=None):\n",
    "    # tf.random_normal([what is the size of your batches, size of output layer])\n",
    "    Weights = tf.Variable(tf.truncated_normal([in_size, out_size], mean=0.1, stddev=0.1))\n",
    "    # tf.random_normal([size of output layer])\n",
    "    biases = tf.Variable(tf.truncated_normal([out_size], mean=0.1, stddev=0.1))\n",
    "    # shape of pred = [size of your batches, size of output layer]\n",
    "    pred = tf.matmul(inputs, Weights) + biases\n",
    "    if activation_function is None:\n",
    "        outputs = pred\n",
    "    else:\n",
    "        outputs = activation_function(pred)\n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_size = 10\n",
    "# placeholders\n",
    "# shape=[how many samples do you have, how many input neurons]\n",
    "x = tf.placeholder(tf.float32, shape=[None, 1], name=\"01_x\")\n",
    "y = tf.placeholder(tf.float32, shape=[None, 1], name=\"01_y\")\n",
    "\n",
    "# drop out\n",
    "keep_prob = tf.placeholder(tf.float32)\n",
    "\n",
    "# create your hidden layers!\n",
    "h1 = add_layer(x, 1, hidden_size, tf.nn.relu)\n",
    "# here is where we shoot down some of the neurons\n",
    "h1_drop = tf.nn.dropout(h1, keep_prob)\n",
    "# add a second layer\n",
    "h2 = add_layer(h1_drop, hidden_size, hidden_size, tf.nn.relu)\n",
    "h2_drop = tf.nn.dropout(h2, keep_prob)\n",
    "# add a third layer\n",
    "h3 = add_layer(h2_drop, hidden_size, hidden_size, tf.nn.relu)\n",
    "h3_drop = tf.nn.dropout(h3, keep_prob)\n",
    "# add a fourth layer\n",
    "h4 = add_layer(h3_drop, hidden_size, hidden_size, tf.nn.relu)\n",
    "h4_drop = tf.nn.dropout(h4, keep_prob)\n",
    "\n",
    "# Output Layers\n",
    "pred = add_layer(h4_drop, hidden_size, 1)\n",
    "\n",
    "# minimize the mean squared errors.\n",
    "loss = tf.reduce_mean(tf.square(pred - y))\n",
    "# pick optimizer\n",
    "optimizer = tf.train.GradientDescentOptimizer(0.01)\n",
    "train = optimizer.minimize(loss)\n",
    "\n",
    "# check accuracy of model\n",
    "correct_prediction = tf.equal(tf.round(pred), tf.round(y))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Best validation accuracy seen so far.\n",
    "best_valid_acc = 0.0\n",
    "# Iteration-number for last improvement to validation accuracy.\n",
    "last_improvement = 0\n",
    "# Stop optimization if no improvement found in this many iterations.\n",
    "require_improvement = 1500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing data points: (1500, 1)\n",
      "Validation data points: (1500, 1) <class 'numpy.ndarray'>\n",
      "Training data points: (7000, 1)\n",
      "Training loss at step 0: 1.140019 \n",
      "Validation 1.127507\n",
      "Training loss at step 100: 0.985999 \n",
      "Validation 0.995302\n",
      "Training loss at step 200: 0.977773 \n",
      "Validation 0.992573\n",
      "Training loss at step 300: 0.963248 \n",
      "Validation 0.987836\n",
      "Training loss at step 400: 0.937316 \n",
      "Validation 0.978585\n",
      "Training loss at step 500: 0.878229 \n",
      "Validation 0.956988\n",
      "Training loss at step 600: 0.692062 \n",
      "Validation 0.890578\n",
      "Training loss at step 700: 0.232598 \n",
      "Validation 0.724202\n",
      "Training loss at step 800: 0.071782 \n",
      "Validation 0.808550\n",
      "Training loss at step 900: 0.048865 \n",
      "Validation 0.880388\n",
      "Training loss at step 1000: 0.043219 \n",
      "Validation 0.897817\n",
      "Training loss at step 1100: 0.042562 \n",
      "Validation 0.897710\n",
      "Training loss at step 1200: 0.040020 \n",
      "Validation 0.903474\n",
      "Training loss at step 1300: 0.040146 \n",
      "Validation 0.902440\n",
      "Training loss at step 1400: 0.041229 \n",
      "Validation 0.899735\n",
      "Training loss at step 1500: 0.039403 \n",
      "Validation 0.901538\n",
      "Training loss at step 1600: 0.036381 \n",
      "Validation 0.903817\n",
      "Training loss at step 1700: 0.037479 \n",
      "Validation 0.907785\n",
      "Training loss at step 1800: 0.037537 \n",
      "Validation 0.904567\n",
      "Training loss at step 1900: 0.036191 \n",
      "Validation 0.904817\n",
      "Training loss at step 2000: 0.038611 \n",
      "Validation 0.902949\n",
      "Training loss at step 2100: 0.039378 \n",
      "Validation 0.901114\n",
      "Training loss at step 2200: 0.037179 \n",
      "Validation 0.902486\n",
      "Training loss at step 2300: 0.035385 \n",
      "Validation 0.904017\n",
      "Training loss at step 2400: 0.036880 \n",
      "Validation 0.901152\n",
      "Training loss at step 2500: 0.036138 \n",
      "Validation 0.902619\n",
      "No improvement found in a while, stopping optimization.\n",
      "Accuracy on the Training Set: 0.88214284\n",
      "Accuracy on the Validation Set: 0.36133334\n",
      "Accuracy on the Test Set: 0.44333333\n",
      "Testing data points: (1500, 1)\n",
      "Validation data points: (1500, 1) <class 'numpy.ndarray'>\n",
      "Training data points: (7000, 1)\n",
      "Training loss at step 0: 0.039307 \n",
      "Validation 1.437072\n",
      "Training loss at step 100: 0.039806 \n",
      "Validation 1.432678\n",
      "Training loss at step 200: 0.039065 \n",
      "Validation 1.435477\n",
      "Training loss at step 300: 0.039912 \n",
      "Validation 1.429169\n",
      "Training loss at step 400: 0.038983 \n",
      "Validation 1.431531\n",
      "Training loss at step 500: 0.038734 \n",
      "Validation 1.433334\n",
      "Training loss at step 600: 0.038791 \n",
      "Validation 1.435287\n",
      "Training loss at step 700: 0.037496 \n",
      "Validation 1.431314\n",
      "Training loss at step 800: 0.039269 \n",
      "Validation 1.432045\n",
      "Training loss at step 900: 0.037938 \n",
      "Validation 1.432473\n",
      "Training loss at step 1000: 0.038837 \n",
      "Validation 1.432220\n",
      "Training loss at step 1100: 0.038072 \n",
      "Validation 1.434807\n",
      "Training loss at step 1200: 0.039662 \n",
      "Validation 1.432357\n",
      "Training loss at step 1300: 0.037456 \n",
      "Validation 1.433775\n",
      "Training loss at step 1400: 0.038575 \n",
      "Validation 1.433113\n",
      "Training loss at step 1500: 0.038214 \n",
      "Validation 1.433317\n",
      "Training loss at step 1600: 0.037811 \n",
      "Validation 1.435319\n",
      "Training loss at step 1700: 0.037960 \n",
      "Validation 1.436453\n",
      "Training loss at step 1800: 0.038924 \n",
      "Validation 1.434200\n",
      "Training loss at step 1900: 0.038838 \n",
      "Validation 1.427214\n",
      "Training loss at step 2000: 0.037636 \n",
      "Validation 1.435332\n",
      "Training loss at step 2100: 0.036191 \n",
      "Validation 1.439013\n",
      "Training loss at step 2200: 0.036316 \n",
      "Validation 1.437431\n",
      "Training loss at step 2300: 0.037471 \n",
      "Validation 1.433908\n",
      "Training loss at step 2400: 0.037853 \n",
      "Validation 1.434120\n",
      "Training loss at step 2500: 0.036689 \n",
      "Validation 1.434120\n",
      "No improvement found in a while, stopping optimization.\n",
      "Accuracy on the Training Set: 0.86457145\n",
      "Accuracy on the Validation Set: 0.34933335\n",
      "Accuracy on the Test Set: 0.432\n",
      "Testing data points: (1500, 1)\n",
      "Validation data points: (1500, 1) <class 'numpy.ndarray'>\n",
      "Training data points: (7000, 1)\n",
      "Training loss at step 0: 0.066693 *\n",
      "Validation 1.026349\n",
      "Training loss at step 100: 0.064178 *\n",
      "Validation 0.989400\n",
      "Training loss at step 200: 0.061393 *\n",
      "Validation 0.987324\n",
      "Training loss at step 300: 0.061816 *\n",
      "Validation 0.986212\n",
      "Training loss at step 400: 0.063490 \n",
      "Validation 0.987531\n",
      "Training loss at step 500: 0.063006 *\n",
      "Validation 0.984614\n",
      "Training loss at step 600: 0.064003 *\n",
      "Validation 0.980973\n",
      "Training loss at step 700: 0.064541 \n",
      "Validation 0.981505\n",
      "Training loss at step 800: 0.061212 \n",
      "Validation 0.983859\n",
      "Training loss at step 900: 0.062147 *\n",
      "Validation 0.980557\n",
      "Training loss at step 1000: 0.060913 \n",
      "Validation 0.982818\n",
      "Training loss at step 1100: 0.064022 \n",
      "Validation 0.981287\n",
      "Training loss at step 1200: 0.063540 \n",
      "Validation 0.981484\n",
      "Training loss at step 1300: 0.062105 *\n",
      "Validation 0.978522\n",
      "Training loss at step 1400: 0.062309 \n",
      "Validation 0.981215\n",
      "Training loss at step 1500: 0.060171 \n",
      "Validation 0.981629\n",
      "Training loss at step 1600: 0.063892 \n",
      "Validation 0.978109\n",
      "Training loss at step 1700: 0.060846 \n",
      "Validation 0.980359\n",
      "Training loss at step 1800: 0.063417 \n",
      "Validation 0.979996\n",
      "Training loss at step 1900: 0.061948 \n",
      "Validation 0.983252\n",
      "Training loss at step 2000: 0.061168 \n",
      "Validation 0.981820\n",
      "Training loss at step 2100: 0.063590 \n",
      "Validation 0.981325\n",
      "Training loss at step 2200: 0.061493 \n",
      "Validation 0.979471\n",
      "Training loss at step 2300: 0.065218 \n",
      "Validation 0.977987\n",
      "Training loss at step 2400: 0.064031 *\n",
      "Validation 0.974502\n",
      "Training loss at step 2500: 0.060559 \n",
      "Validation 0.977567\n",
      "Training loss at step 2600: 0.061762 \n",
      "Validation 0.979668\n",
      "Training loss at step 2700: 0.061901 \n",
      "Validation 0.977376\n",
      "Training loss at step 2800: 0.061711 \n",
      "Validation 0.977147\n",
      "Training loss at step 2900: 0.060030 \n",
      "Validation 0.980246\n",
      "Training loss at step 3000: 0.061460 \n",
      "Validation 0.980415\n",
      "Training loss at step 3100: 0.062569 \n",
      "Validation 0.979993\n",
      "Training loss at step 3200: 0.062532 \n",
      "Validation 0.979970\n",
      "Training loss at step 3300: 0.061992 \n",
      "Validation 0.982500\n",
      "Training loss at step 3400: 0.060628 \n",
      "Validation 0.978430\n",
      "Training loss at step 3500: 0.059542 \n",
      "Validation 0.977575\n",
      "Training loss at step 3600: 0.062071 \n",
      "Validation 0.978755\n",
      "Training loss at step 3700: 0.062435 \n",
      "Validation 0.981008\n",
      "Training loss at step 3800: 0.063084 \n",
      "Validation 0.975950\n",
      "Training loss at step 3900: 0.060144 \n",
      "Validation 0.978786\n",
      "Training loss at step 4000: 0.061052 \n",
      "Validation 0.981888\n",
      "No improvement found in a while, stopping optimization.\n",
      "Accuracy on the Training Set: 0.844\n",
      "Accuracy on the Validation Set: 0.586\n",
      "Accuracy on the Test Set: 0.39066666\n",
      "Testing data points: (1500, 1)\n",
      "Validation data points: (1500, 1) <class 'numpy.ndarray'>\n",
      "Training data points: (7000, 1)\n",
      "Training loss at step 0: 0.041530 \n",
      "Validation 1.355002\n",
      "Training loss at step 100: 0.037925 \n",
      "Validation 1.462434\n",
      "Training loss at step 200: 0.035981 \n",
      "Validation 1.482274\n",
      "Training loss at step 300: 0.036451 \n",
      "Validation 1.482851\n",
      "Training loss at step 400: 0.036827 \n",
      "Validation 1.484785\n",
      "Training loss at step 500: 0.035776 \n",
      "Validation 1.489882\n",
      "Training loss at step 600: 0.035010 \n",
      "Validation 1.487806\n",
      "Training loss at step 700: 0.034965 \n",
      "Validation 1.491202\n",
      "Training loss at step 800: 0.035502 \n",
      "Validation 1.485274\n",
      "Training loss at step 900: 0.034683 \n",
      "Validation 1.486432\n",
      "Training loss at step 1000: 0.035025 \n",
      "Validation 1.486395\n",
      "Training loss at step 1100: 0.035124 \n",
      "Validation 1.491762\n",
      "Training loss at step 1200: 0.035689 \n",
      "Validation 1.482066\n",
      "Training loss at step 1300: 0.034964 \n",
      "Validation 1.484996\n",
      "Training loss at step 1400: 0.035575 \n",
      "Validation 1.483177\n",
      "Training loss at step 1500: 0.035453 \n",
      "Validation 1.481565\n",
      "Training loss at step 1600: 0.036257 \n",
      "Validation 1.484139\n",
      "Training loss at step 1700: 0.035489 \n",
      "Validation 1.480571\n",
      "Training loss at step 1800: 0.034134 \n",
      "Validation 1.486340\n",
      "Training loss at step 1900: 0.036459 \n",
      "Validation 1.478164\n",
      "Training loss at step 2000: 0.035466 \n",
      "Validation 1.487942\n",
      "Training loss at step 2100: 0.036500 \n",
      "Validation 1.482751\n",
      "Training loss at step 2200: 0.034284 \n",
      "Validation 1.484544\n",
      "Training loss at step 2300: 0.035898 \n",
      "Validation 1.479825\n",
      "Training loss at step 2400: 0.034772 \n",
      "Validation 1.488153\n",
      "Training loss at step 2500: 0.034771 \n",
      "Validation 1.484915\n",
      "Training loss at step 2600: 0.034925 \n",
      "Validation 1.482534\n",
      "Training loss at step 2700: 0.034396 \n",
      "Validation 1.484203\n",
      "Training loss at step 2800: 0.034330 \n",
      "Validation 1.487538\n",
      "Training loss at step 2900: 0.033669 \n",
      "Validation 1.493042\n",
      "Training loss at step 3000: 0.033496 \n",
      "Validation 1.486786\n",
      "Training loss at step 3100: 0.035298 \n",
      "Validation 1.483227\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss at step 3200: 0.036300 \n",
      "Validation 1.482343\n",
      "Training loss at step 3300: 0.035650 \n",
      "Validation 1.483997\n",
      "Training loss at step 3400: 0.034978 \n",
      "Validation 1.486492\n",
      "Training loss at step 3500: 0.035949 \n",
      "Validation 1.484438\n",
      "Training loss at step 3600: 0.034261 \n",
      "Validation 1.486522\n",
      "Training loss at step 3700: 0.033733 \n",
      "Validation 1.488932\n",
      "Training loss at step 3800: 0.035216 \n",
      "Validation 1.488672\n",
      "Training loss at step 3900: 0.034580 \n",
      "Validation 1.485030\n",
      "Training loss at step 4000: 0.033654 \n",
      "Validation 1.490167\n",
      "No improvement found in a while, stopping optimization.\n",
      "Accuracy on the Training Set: 0.85714287\n",
      "Accuracy on the Validation Set: 0.41666666\n",
      "Accuracy on the Test Set: 0.46733335\n",
      "Testing data points: (1500, 1)\n",
      "Validation data points: (1500, 1) <class 'numpy.ndarray'>\n",
      "Training data points: (7000, 1)\n",
      "Training loss at step 0: 0.045135 \n",
      "Validation 1.189311\n",
      "Training loss at step 100: 0.042808 \n",
      "Validation 1.145568\n",
      "Training loss at step 200: 0.043866 \n",
      "Validation 1.143491\n",
      "Training loss at step 300: 0.042027 \n",
      "Validation 1.149713\n",
      "Training loss at step 400: 0.042643 \n",
      "Validation 1.147812\n",
      "Training loss at step 500: 0.043249 \n",
      "Validation 1.146796\n",
      "Training loss at step 600: 0.041437 \n",
      "Validation 1.145572\n",
      "Training loss at step 700: 0.041396 \n",
      "Validation 1.145869\n",
      "Training loss at step 800: 0.040826 \n",
      "Validation 1.147727\n",
      "Training loss at step 900: 0.043670 \n",
      "Validation 1.145313\n",
      "Training loss at step 1000: 0.041355 \n",
      "Validation 1.145749\n",
      "Training loss at step 1100: 0.042392 \n",
      "Validation 1.148254\n",
      "Training loss at step 1200: 0.041728 \n",
      "Validation 1.145586\n",
      "Training loss at step 1300: 0.040922 \n",
      "Validation 1.144966\n",
      "Training loss at step 1400: 0.042171 \n",
      "Validation 1.148315\n",
      "Training loss at step 1500: 0.043858 \n",
      "Validation 1.146375\n",
      "Training loss at step 1600: 0.043086 \n",
      "Validation 1.148598\n",
      "Training loss at step 1700: 0.041429 \n",
      "Validation 1.145608\n",
      "Training loss at step 1800: 0.041364 \n",
      "Validation 1.143946\n",
      "Training loss at step 1900: 0.042904 \n",
      "Validation 1.145742\n",
      "Training loss at step 2000: 0.040716 \n",
      "Validation 1.148857\n",
      "Training loss at step 2100: 0.040913 \n",
      "Validation 1.145451\n",
      "Training loss at step 2200: 0.041251 \n",
      "Validation 1.146157\n",
      "Training loss at step 2300: 0.041362 \n",
      "Validation 1.148683\n",
      "Training loss at step 2400: 0.040422 \n",
      "Validation 1.146568\n",
      "Training loss at step 2500: 0.042015 \n",
      "Validation 1.148366\n",
      "Training loss at step 2600: 0.040791 \n",
      "Validation 1.142999\n",
      "Training loss at step 2700: 0.042380 \n",
      "Validation 1.143481\n",
      "Training loss at step 2800: 0.042663 \n",
      "Validation 1.144757\n",
      "Training loss at step 2900: 0.042010 \n",
      "Validation 1.146430\n",
      "Training loss at step 3000: 0.042780 \n",
      "Validation 1.147296\n",
      "Training loss at step 3100: 0.041710 \n",
      "Validation 1.146789\n",
      "Training loss at step 3200: 0.041233 \n",
      "Validation 1.148191\n",
      "Training loss at step 3300: 0.041965 \n",
      "Validation 1.145447\n",
      "Training loss at step 3400: 0.042300 \n",
      "Validation 1.145391\n",
      "Training loss at step 3500: 0.042878 \n",
      "Validation 1.146334\n",
      "Training loss at step 3600: 0.042117 \n",
      "Validation 1.145884\n",
      "Training loss at step 3700: 0.043065 \n",
      "Validation 1.143746\n",
      "Training loss at step 3800: 0.042397 \n",
      "Validation 1.146272\n",
      "Training loss at step 3900: 0.042566 \n",
      "Validation 1.149300\n",
      "Training loss at step 4000: 0.041195 \n",
      "Validation 1.145597\n",
      "No improvement found in a while, stopping optimization.\n",
      "Accuracy on the Training Set: 0.8988571\n",
      "Accuracy on the Validation Set: 0.41133332\n",
      "Accuracy on the Test Set: 0.42133334\n",
      "Testing data points: (1500, 1)\n",
      "Validation data points: (1500, 1) <class 'numpy.ndarray'>\n",
      "Training data points: (7000, 1)\n",
      "Training loss at step 0: 0.099298 \n",
      "Validation 1.556373\n",
      "Training loss at step 100: 0.097369 \n",
      "Validation 1.599208\n",
      "Training loss at step 200: 0.098289 \n",
      "Validation 1.608611\n",
      "Training loss at step 300: 0.097024 \n",
      "Validation 1.609110\n",
      "Training loss at step 400: 0.095659 \n",
      "Validation 1.612065\n",
      "Training loss at step 500: 0.096448 \n",
      "Validation 1.612033\n",
      "Training loss at step 600: 0.096864 \n",
      "Validation 1.613045\n",
      "Training loss at step 700: 0.098563 \n",
      "Validation 1.610901\n",
      "Training loss at step 800: 0.099250 \n",
      "Validation 1.611538\n",
      "Training loss at step 900: 0.096661 \n",
      "Validation 1.610873\n",
      "Training loss at step 1000: 0.095662 \n",
      "Validation 1.612777\n",
      "Training loss at step 1100: 0.095635 \n",
      "Validation 1.610402\n",
      "Training loss at step 1200: 0.095718 \n",
      "Validation 1.612489\n",
      "Training loss at step 1300: 0.097501 \n",
      "Validation 1.609917\n",
      "Training loss at step 1400: 0.096550 \n",
      "Validation 1.612132\n",
      "Training loss at step 1500: 0.096866 \n",
      "Validation 1.611698\n",
      "Training loss at step 1600: 0.096122 \n",
      "Validation 1.609299\n",
      "Training loss at step 1700: 0.098009 \n",
      "Validation 1.610510\n",
      "Training loss at step 1800: 0.097219 \n",
      "Validation 1.607077\n",
      "Training loss at step 1900: 0.096520 \n",
      "Validation 1.611247\n",
      "Training loss at step 2000: 0.095822 \n",
      "Validation 1.607850\n",
      "Training loss at step 2100: 0.095481 \n",
      "Validation 1.613930\n",
      "Training loss at step 2200: 0.096003 \n",
      "Validation 1.610225\n",
      "Training loss at step 2300: 0.095697 \n",
      "Validation 1.615186\n",
      "Training loss at step 2400: 0.098425 \n",
      "Validation 1.612336\n",
      "Training loss at step 2500: 0.096459 \n",
      "Validation 1.615085\n",
      "Training loss at step 2600: 0.097226 \n",
      "Validation 1.609542\n",
      "Training loss at step 2700: 0.096229 \n",
      "Validation 1.610889\n",
      "Training loss at step 2800: 0.098206 \n",
      "Validation 1.610544\n",
      "Training loss at step 2900: 0.096079 \n",
      "Validation 1.605174\n",
      "Training loss at step 3000: 0.096918 \n",
      "Validation 1.614169\n",
      "Training loss at step 3100: 0.098451 \n",
      "Validation 1.614200\n",
      "Training loss at step 3200: 0.096562 \n",
      "Validation 1.611395\n",
      "Training loss at step 3300: 0.096545 \n",
      "Validation 1.610816\n",
      "Training loss at step 3400: 0.097264 \n",
      "Validation 1.610444\n",
      "Training loss at step 3500: 0.096593 \n",
      "Validation 1.614917\n",
      "Training loss at step 3600: 0.095538 \n",
      "Validation 1.608936\n",
      "Training loss at step 3700: 0.098324 \n",
      "Validation 1.614099\n",
      "Training loss at step 3800: 0.094997 \n",
      "Validation 1.613720\n",
      "Training loss at step 3900: 0.095627 \n",
      "Validation 1.606512\n",
      "Training loss at step 4000: 0.094586 \n",
      "Validation 1.606788\n",
      "No improvement found in a while, stopping optimization.\n",
      "Accuracy on the Training Set: 0.73014283\n",
      "Accuracy on the Validation Set: 0.41666666\n",
      "Accuracy on the Test Set: 0.504\n",
      "Testing data points: (1500, 1)\n",
      "Validation data points: (1500, 1) <class 'numpy.ndarray'>\n",
      "Training data points: (7000, 1)\n",
      "Training loss at step 0: 0.117832 \n",
      "Validation 1.847839\n",
      "Training loss at step 100: 0.076392 \n",
      "Validation 1.727007\n",
      "Training loss at step 200: 0.077911 \n",
      "Validation 1.734612\n",
      "Training loss at step 300: 0.077605 \n",
      "Validation 1.722780\n",
      "Training loss at step 400: 0.079329 \n",
      "Validation 1.735443\n",
      "Training loss at step 500: 0.076158 \n",
      "Validation 1.741232\n",
      "Training loss at step 600: 0.078921 \n",
      "Validation 1.725016\n",
      "Training loss at step 700: 0.077702 \n",
      "Validation 1.722742\n",
      "Training loss at step 800: 0.078301 \n",
      "Validation 1.719236\n",
      "Training loss at step 900: 0.078056 \n",
      "Validation 1.722258\n",
      "Training loss at step 1000: 0.077827 \n",
      "Validation 1.725123\n",
      "Training loss at step 1100: 0.079150 \n",
      "Validation 1.723379\n",
      "Training loss at step 1200: 0.077704 \n",
      "Validation 1.743378\n",
      "Training loss at step 1300: 0.079063 \n",
      "Validation 1.738346\n",
      "Training loss at step 1400: 0.077718 \n",
      "Validation 1.722677\n",
      "Training loss at step 1500: 0.078495 \n",
      "Validation 1.730103\n",
      "Training loss at step 1600: 0.078558 \n",
      "Validation 1.744090\n",
      "Training loss at step 1700: 0.076332 \n",
      "Validation 1.719672\n",
      "Training loss at step 1800: 0.077295 \n",
      "Validation 1.732369\n",
      "Training loss at step 1900: 0.075563 \n",
      "Validation 1.747150\n",
      "Training loss at step 2000: 0.077059 \n",
      "Validation 1.734028\n",
      "Training loss at step 2100: 0.076832 \n",
      "Validation 1.746456\n",
      "Training loss at step 2200: 0.077435 \n",
      "Validation 1.736420\n",
      "Training loss at step 2300: 0.079017 \n",
      "Validation 1.741706\n",
      "Training loss at step 2400: 0.076555 \n",
      "Validation 1.744571\n",
      "Training loss at step 2500: 0.076773 \n",
      "Validation 1.743759\n",
      "Training loss at step 2600: 0.079053 \n",
      "Validation 1.722607\n",
      "Training loss at step 2700: 0.076588 \n",
      "Validation 1.729743\n",
      "Training loss at step 2800: 0.077951 \n",
      "Validation 1.717330\n",
      "Training loss at step 2900: 0.076860 \n",
      "Validation 1.742094\n",
      "Training loss at step 3000: 0.078394 \n",
      "Validation 1.744156\n",
      "Training loss at step 3100: 0.078439 \n",
      "Validation 1.742119\n",
      "Training loss at step 3200: 0.077867 \n",
      "Validation 1.731532\n",
      "Training loss at step 3300: 0.077292 \n",
      "Validation 1.725298\n",
      "Training loss at step 3400: 0.079225 \n",
      "Validation 1.720044\n",
      "Training loss at step 3500: 0.077290 \n",
      "Validation 1.723500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss at step 3600: 0.077408 \n",
      "Validation 1.743347\n",
      "Training loss at step 3700: 0.079622 \n",
      "Validation 1.746759\n",
      "Training loss at step 3800: 0.077771 \n",
      "Validation 1.745658\n",
      "Training loss at step 3900: 0.077770 \n",
      "Validation 1.742620\n",
      "Training loss at step 4000: 0.077736 \n",
      "Validation 1.726792\n",
      "No improvement found in a while, stopping optimization.\n",
      "Accuracy on the Training Set: 0.7814286\n",
      "Accuracy on the Validation Set: 0.48333332\n",
      "Accuracy on the Test Set: 0.5273333\n",
      "Testing data points: (1500, 1)\n",
      "Validation data points: (1500, 1) <class 'numpy.ndarray'>\n",
      "Training data points: (7000, 1)\n",
      "Training loss at step 0: 0.047856 \n",
      "Validation 1.962086\n",
      "Training loss at step 100: 0.032429 \n",
      "Validation 2.085629\n",
      "Training loss at step 200: 0.032650 \n",
      "Validation 2.109011\n",
      "Training loss at step 300: 0.031929 \n",
      "Validation 2.110198\n",
      "Training loss at step 400: 0.032911 \n",
      "Validation 2.116784\n",
      "Training loss at step 500: 0.031465 \n",
      "Validation 2.120842\n",
      "Training loss at step 600: 0.033065 \n",
      "Validation 2.116729\n",
      "Training loss at step 700: 0.030199 \n",
      "Validation 2.124468\n",
      "Training loss at step 800: 0.030029 \n",
      "Validation 2.124805\n",
      "Training loss at step 900: 0.030836 \n",
      "Validation 2.121276\n",
      "Training loss at step 1000: 0.031949 \n",
      "Validation 2.124248\n",
      "Training loss at step 1100: 0.030800 \n",
      "Validation 2.122511\n",
      "Training loss at step 1200: 0.030559 \n",
      "Validation 2.119854\n",
      "Training loss at step 1300: 0.031092 \n",
      "Validation 2.122951\n",
      "Training loss at step 1400: 0.032131 \n",
      "Validation 2.123116\n",
      "Training loss at step 1500: 0.030639 \n",
      "Validation 2.121875\n",
      "Training loss at step 1600: 0.030310 \n",
      "Validation 2.124397\n",
      "Training loss at step 1700: 0.031368 \n",
      "Validation 2.123554\n",
      "Training loss at step 1800: 0.031713 \n",
      "Validation 2.127972\n",
      "Training loss at step 1900: 0.031819 \n",
      "Validation 2.123613\n",
      "Training loss at step 2000: 0.030708 \n",
      "Validation 2.129676\n",
      "Training loss at step 2100: 0.030631 \n",
      "Validation 2.125509\n",
      "Training loss at step 2200: 0.030936 \n",
      "Validation 2.120955\n",
      "Training loss at step 2300: 0.030763 \n",
      "Validation 2.121319\n",
      "Training loss at step 2400: 0.030986 \n",
      "Validation 2.120589\n",
      "Training loss at step 2500: 0.033155 \n",
      "Validation 2.123992\n",
      "Training loss at step 2600: 0.030526 \n",
      "Validation 2.120528\n",
      "Training loss at step 2700: 0.030096 \n",
      "Validation 2.122404\n",
      "Training loss at step 2800: 0.030602 \n",
      "Validation 2.121972\n",
      "Training loss at step 2900: 0.031106 \n",
      "Validation 2.125216\n",
      "Training loss at step 3000: 0.030254 \n",
      "Validation 2.121762\n",
      "Training loss at step 3100: 0.030938 \n",
      "Validation 2.123801\n",
      "Training loss at step 3200: 0.031187 \n",
      "Validation 2.125886\n",
      "Training loss at step 3300: 0.030850 \n",
      "Validation 2.122269\n",
      "Training loss at step 3400: 0.031539 \n",
      "Validation 2.125500\n",
      "Training loss at step 3500: 0.030355 \n",
      "Validation 2.124007\n",
      "Training loss at step 3600: 0.029190 \n",
      "Validation 2.126407\n",
      "Training loss at step 3700: 0.031746 \n",
      "Validation 2.124508\n",
      "Training loss at step 3800: 0.031490 \n",
      "Validation 2.118787\n",
      "Training loss at step 3900: 0.030632 \n",
      "Validation 2.125211\n",
      "Training loss at step 4000: 0.031789 \n",
      "Validation 2.121499\n",
      "No improvement found in a while, stopping optimization.\n",
      "Accuracy on the Training Set: 0.87057143\n",
      "Accuracy on the Validation Set: 0.39066666\n",
      "Accuracy on the Test Set: 0.432\n",
      "Testing data points: (1500, 1)\n",
      "Validation data points: (1500, 1) <class 'numpy.ndarray'>\n",
      "Training data points: (7000, 1)\n",
      "Training loss at step 0: 0.056494 \n",
      "Validation 1.186532\n",
      "Training loss at step 100: 0.052019 \n",
      "Validation 1.105632\n",
      "Training loss at step 200: 0.052026 \n",
      "Validation 1.105969\n",
      "Training loss at step 300: 0.050223 \n",
      "Validation 1.103621\n",
      "Training loss at step 400: 0.050153 \n",
      "Validation 1.105294\n",
      "Training loss at step 500: 0.050253 \n",
      "Validation 1.102697\n",
      "Training loss at step 600: 0.050725 \n",
      "Validation 1.105702\n",
      "Training loss at step 700: 0.049847 \n",
      "Validation 1.108919\n",
      "Training loss at step 800: 0.048737 \n",
      "Validation 1.101306\n",
      "Training loss at step 900: 0.050877 \n",
      "Validation 1.108461\n",
      "Training loss at step 1000: 0.052967 \n",
      "Validation 1.104585\n",
      "Training loss at step 1100: 0.051738 \n",
      "Validation 1.104998\n",
      "Training loss at step 1200: 0.050619 \n",
      "Validation 1.103237\n",
      "Training loss at step 1300: 0.051012 \n",
      "Validation 1.101803\n",
      "Training loss at step 1400: 0.050926 \n",
      "Validation 1.107171\n",
      "Training loss at step 1500: 0.049889 \n",
      "Validation 1.107874\n",
      "Training loss at step 1600: 0.051296 \n",
      "Validation 1.100503\n",
      "Training loss at step 1700: 0.051072 \n",
      "Validation 1.102854\n",
      "Training loss at step 1800: 0.051911 \n",
      "Validation 1.104408\n",
      "Training loss at step 1900: 0.051228 \n",
      "Validation 1.101997\n",
      "Training loss at step 2000: 0.052145 \n",
      "Validation 1.107593\n",
      "Training loss at step 2100: 0.051496 \n",
      "Validation 1.107427\n",
      "Training loss at step 2200: 0.050623 \n",
      "Validation 1.102736\n",
      "Training loss at step 2300: 0.050502 \n",
      "Validation 1.103505\n",
      "Training loss at step 2400: 0.050381 \n",
      "Validation 1.103653\n",
      "Training loss at step 2500: 0.050853 \n",
      "Validation 1.102258\n",
      "Training loss at step 2600: 0.051117 \n",
      "Validation 1.107302\n",
      "Training loss at step 2700: 0.052020 \n",
      "Validation 1.106969\n",
      "Training loss at step 2800: 0.048850 \n",
      "Validation 1.106520\n",
      "Training loss at step 2900: 0.051090 \n",
      "Validation 1.105820\n",
      "Training loss at step 3000: 0.052539 \n",
      "Validation 1.102910\n",
      "Training loss at step 3100: 0.050943 \n",
      "Validation 1.104086\n",
      "Training loss at step 3200: 0.049560 \n",
      "Validation 1.102802\n",
      "Training loss at step 3300: 0.050621 \n",
      "Validation 1.105123\n",
      "Training loss at step 3400: 0.049343 \n",
      "Validation 1.107892\n",
      "Training loss at step 3500: 0.052679 \n",
      "Validation 1.100246\n",
      "Training loss at step 3600: 0.050812 \n",
      "Validation 1.108485\n",
      "Training loss at step 3700: 0.050169 \n",
      "Validation 1.107751\n",
      "Training loss at step 3800: 0.050605 \n",
      "Validation 1.102513\n",
      "Training loss at step 3900: 0.050315 \n",
      "Validation 1.102099\n",
      "Training loss at step 4000: 0.052275 \n",
      "Validation 1.103107\n",
      "No improvement found in a while, stopping optimization.\n",
      "Accuracy on the Training Set: 0.8462857\n",
      "Accuracy on the Validation Set: 0.34866667\n",
      "Accuracy on the Test Set: 0.44266668\n",
      "Testing data points: (1500, 1)\n",
      "Validation data points: (1500, 1) <class 'numpy.ndarray'>\n",
      "Training data points: (7000, 1)\n",
      "Training loss at step 0: 0.046325 \n",
      "Validation 1.518310\n",
      "Training loss at step 100: 0.036090 \n",
      "Validation 1.663723\n",
      "Training loss at step 200: 0.037317 \n",
      "Validation 1.685264\n",
      "Training loss at step 300: 0.036783 \n",
      "Validation 1.688900\n",
      "Training loss at step 400: 0.036140 \n",
      "Validation 1.686694\n",
      "Training loss at step 500: 0.036083 \n",
      "Validation 1.686043\n",
      "Training loss at step 600: 0.036280 \n",
      "Validation 1.688184\n",
      "Training loss at step 700: 0.035969 \n",
      "Validation 1.684728\n",
      "Training loss at step 800: 0.037373 \n",
      "Validation 1.688473\n",
      "Training loss at step 900: 0.034600 \n",
      "Validation 1.690536\n",
      "Training loss at step 1000: 0.036380 \n",
      "Validation 1.687035\n",
      "Training loss at step 1100: 0.036380 \n",
      "Validation 1.685738\n",
      "Training loss at step 1200: 0.036848 \n",
      "Validation 1.693048\n",
      "Training loss at step 1300: 0.036713 \n",
      "Validation 1.687138\n",
      "Training loss at step 1400: 0.034835 \n",
      "Validation 1.690864\n",
      "Training loss at step 1500: 0.034915 \n",
      "Validation 1.689474\n",
      "Training loss at step 1600: 0.036353 \n",
      "Validation 1.689228\n",
      "Training loss at step 1700: 0.035281 \n",
      "Validation 1.685814\n",
      "Training loss at step 1800: 0.037494 \n",
      "Validation 1.688734\n",
      "Training loss at step 1900: 0.035825 \n",
      "Validation 1.686439\n",
      "Training loss at step 2000: 0.036181 \n",
      "Validation 1.685513\n",
      "Training loss at step 2100: 0.037683 \n",
      "Validation 1.687500\n",
      "Training loss at step 2200: 0.036870 \n",
      "Validation 1.687820\n",
      "Training loss at step 2300: 0.034447 \n",
      "Validation 1.688740\n",
      "Training loss at step 2400: 0.036074 \n",
      "Validation 1.687390\n",
      "Training loss at step 2500: 0.035072 \n",
      "Validation 1.691425\n",
      "Training loss at step 2600: 0.037047 \n",
      "Validation 1.683412\n",
      "Training loss at step 2700: 0.037718 \n",
      "Validation 1.685385\n",
      "Training loss at step 2800: 0.036295 \n",
      "Validation 1.688183\n",
      "Training loss at step 2900: 0.036191 \n",
      "Validation 1.685038\n",
      "Training loss at step 3000: 0.036530 \n",
      "Validation 1.686931\n",
      "Training loss at step 3100: 0.036755 \n",
      "Validation 1.686793\n",
      "Training loss at step 3200: 0.035931 \n",
      "Validation 1.684245\n",
      "Training loss at step 3300: 0.036613 \n",
      "Validation 1.689292\n",
      "Training loss at step 3400: 0.035249 \n",
      "Validation 1.685529\n",
      "Training loss at step 3500: 0.036548 \n",
      "Validation 1.690464\n",
      "Training loss at step 3600: 0.038628 \n",
      "Validation 1.681206\n",
      "Training loss at step 3700: 0.036269 \n",
      "Validation 1.692221\n",
      "Training loss at step 3800: 0.036053 \n",
      "Validation 1.686623\n",
      "Training loss at step 3900: 0.037603 \n",
      "Validation 1.678654\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss at step 4000: 0.035817 \n",
      "Validation 1.689527\n",
      "No improvement found in a while, stopping optimization.\n",
      "Accuracy on the Training Set: 0.8371429\n",
      "Accuracy on the Validation Set: 0.43066666\n",
      "Accuracy on the Test Set: 0.5086667\n"
     ]
    }
   ],
   "source": [
    "# initialize the variables\n",
    "init = tf.global_variables_initializer()\n",
    "# hold step and error values\n",
    "t = []\n",
    "size_list = [i for i in range(100, 10000, 1000)]\n",
    "accuracies = []\n",
    "# Run your graph\n",
    "with tf.Session() as sess:\n",
    "    # initialize variables\n",
    "    sess.run(init)\n",
    "    for index in size_list:\n",
    "        train_x, train_y,valid_x,valid_y,test_x,test_y = generate_experiment_data(index)\n",
    "        # Fit the function.\n",
    "        for step in range(6000):\n",
    "            # get your data\n",
    "            train_data = {x:train_x, y:train_y, keep_prob: 0.975}\n",
    "            valid_data = {x:valid_x, y:valid_y, keep_prob: 1.0}\n",
    "            test_data = {x:test_x, y:test_y, keep_prob: 1.0}\n",
    "            # training in progress...\n",
    "            train_loss, train_pred = sess.run([loss, train], feed_dict=train_data)        \n",
    "            # print every n iterations\n",
    "            if step%100==0:\n",
    "                # capture the step and error for analysis\n",
    "                valid_loss = sess.run(loss, feed_dict=valid_data) \n",
    "                t.append((step, train_loss, valid_loss))    \n",
    "                # get snapshot of current training and validation accuracy       \n",
    "                train_acc = accuracy.eval(train_data)\n",
    "                valid_acc = accuracy.eval(valid_data)           \n",
    "                # If validation accuracy is an improvement over best-known.\n",
    "                if valid_acc > best_valid_acc:\n",
    "                    # Update the best-known validation accuracy.\n",
    "                    best_valid_acc = valid_acc\n",
    "                    # Set the iteration for the last improvement to current.\n",
    "                    last_improvement = step\n",
    "                    # Flag when ever an improvement is found\n",
    "                    improved_str = '*'\n",
    "                else:\n",
    "                    # An empty string to be printed below.\n",
    "                    # Shows that no improvement was found.\n",
    "                    improved_str = ''   \n",
    "\n",
    "                print(\"Training loss at step %d: %f %s\" % (step, train_loss, improved_str))        \n",
    "                print(\"Validation %f\" % (valid_loss))            \n",
    "\n",
    "                # If no improvement found in the required number of iterations.\n",
    "                if step - last_improvement > require_improvement:\n",
    "                    print(\"No improvement found in a while, stopping optimization.\")\n",
    "                    # Break out from the for-loop.\n",
    "                    break                \n",
    "\n",
    "        # here is where you see how good of a Data Scientist you are        \n",
    "        print(\"Accuracy on the Training Set:\", accuracy.eval(train_data))\n",
    "        print(\"Accuracy on the Validation Set:\", accuracy.eval(valid_data)) \n",
    "        print(\"Accuracy on the Test Set:\", accuracy.eval(test_data))\n",
    "        accuracies.append(accuracy.eval(train_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.8798571, 0.853, 0.8447143, 0.85728574, 0.9035714, 0.7245714, 0.77842855, 0.8765714, 0.851, 0.8462857]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.84152853\n"
     ]
    }
   ],
   "source": [
    "print(accuracies)\n",
    "plt.plot(size_list, accuracies)\n",
    "plt.xlabel('Size')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.show()\n",
    "print(np.average(accuracies))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
