{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import gym\n",
    "\n",
    "np.random.seed(2)\n",
    "tf.set_random_seed(2)  # reproducible\n",
    "\n",
    "# Superparameters\n",
    "OUTPUT_GRAPH = False\n",
    "MAX_EPISODE = 3000\n",
    "DISPLAY_REWARD_THRESHOLD = 200  # renders environment if total episode reward is greater then this threshold\n",
    "MAX_EP_STEPS = 1000   # maximum time step in one episode\n",
    "RENDER = False  # rendering wastes time\n",
    "GAMMA = 0.9     # reward discount in TD error\n",
    "LR_A = 0.001    # learning rate for actor\n",
    "LR_C = 0.01     # learning rate for critic\n",
    "\n",
    "env = gym.make('CartPole-v0')\n",
    "env.seed(1)  # reproducible\n",
    "env = env.unwrapped\n",
    "\n",
    "N_F = env.observation_space.shape[0]\n",
    "N_A = env.action_space.n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Actor(object):\n",
    "    def __init__(self, sess, n_features, n_actions, lr=0.001):\n",
    "        self.sess = sess\n",
    "\n",
    "        self.s = tf.placeholder(tf.float32, [1, n_features], \"state\")\n",
    "        self.a = tf.placeholder(tf.int32, None, \"act\")\n",
    "        self.td_error = tf.placeholder(tf.float32, None, \"td_error\")  # TD_error\n",
    "\n",
    "        with tf.variable_scope('Actor'):\n",
    "            l1 = tf.layers.dense(\n",
    "                inputs=self.s,\n",
    "                units=20,    # number of hidden units\n",
    "                activation=tf.nn.relu,\n",
    "                kernel_initializer=tf.random_normal_initializer(0., .1),    # weights\n",
    "                bias_initializer=tf.constant_initializer(0.1),  # biases\n",
    "                name='l1'\n",
    "            )\n",
    "\n",
    "            self.acts_prob = tf.layers.dense(\n",
    "                inputs=l1,\n",
    "                units=n_actions,    # output units\n",
    "                activation=tf.nn.softmax,   # get action probabilities\n",
    "                kernel_initializer=tf.random_normal_initializer(0., .1),  # weights\n",
    "                bias_initializer=tf.constant_initializer(0.1),  # biases\n",
    "                name='acts_prob'\n",
    "            )\n",
    "\n",
    "        with tf.variable_scope('exp_v'):\n",
    "            log_prob = tf.log(self.acts_prob[0, self.a])\n",
    "            self.exp_v = tf.reduce_mean(log_prob * self.td_error)  # advantage (TD_error) guided loss\n",
    "\n",
    "        with tf.variable_scope('train'):\n",
    "            self.train_op = tf.train.AdamOptimizer(lr).minimize(-self.exp_v)  # minimize(-exp_v) = maximize(exp_v)\n",
    "\n",
    "    def learn(self, s, a, td):\n",
    "        s = s[np.newaxis, :]\n",
    "        feed_dict = {self.s: s, self.a: a, self.td_error: td}\n",
    "        _, exp_v = self.sess.run([self.train_op, self.exp_v], feed_dict)\n",
    "        return exp_v\n",
    "\n",
    "    def choose_action(self, s):\n",
    "        s = s[np.newaxis, :]\n",
    "        probs = self.sess.run(self.acts_prob, {self.s: s})   # get probabilities for all actions\n",
    "        return np.random.choice(np.arange(probs.shape[1]), p=probs.ravel()) # return a int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Critic(object):\n",
    "    def __init__(self, sess, n_features, lr=0.01):\n",
    "        self.sess = sess\n",
    "\n",
    "        self.s = tf.placeholder(tf.float32, [1, n_features], \"state\")\n",
    "        self.v_ = tf.placeholder(tf.float32, [1, 1], \"v_next\")\n",
    "        self.r = tf.placeholder(tf.float32, None, 'r')\n",
    "\n",
    "        with tf.variable_scope('Critic'):\n",
    "            l1 = tf.layers.dense(\n",
    "                inputs=self.s,\n",
    "                units=20,  # number of hidden units\n",
    "                activation=tf.nn.relu,  # None\n",
    "                # have to be linear to make sure the convergence of actor.\n",
    "                # But linear approximator seems hardly learns the correct Q.\n",
    "                kernel_initializer=tf.random_normal_initializer(0., .1),  # weights\n",
    "                bias_initializer=tf.constant_initializer(0.1),  # biases\n",
    "                name='l1'\n",
    "            )\n",
    "\n",
    "            self.v = tf.layers.dense(\n",
    "                inputs=l1,\n",
    "                units=1,  # output units\n",
    "                activation=None,\n",
    "                kernel_initializer=tf.random_normal_initializer(0., .1),  # weights\n",
    "                bias_initializer=tf.constant_initializer(0.1),  # biases\n",
    "                name='V'\n",
    "            )\n",
    "\n",
    "        with tf.variable_scope('squared_TD_error'):\n",
    "            self.td_error = self.r + GAMMA * self.v_ - self.v\n",
    "            self.loss = tf.square(self.td_error)    # TD_error = (r+gamma*V_next) - V_eval\n",
    "        with tf.variable_scope('train'):\n",
    "            self.train_op = tf.train.AdamOptimizer(lr).minimize(self.loss)\n",
    "\n",
    "    def learn(self, s, r, s_):\n",
    "        s, s_ = s[np.newaxis, :], s_[np.newaxis, :]\n",
    "\n",
    "        v_ = self.sess.run(self.v, {self.s: s_})\n",
    "        td_error, _ = self.sess.run([self.td_error, self.train_op],\n",
    "                                          {self.s: s, self.v_: v_, self.r: r})\n",
    "        return td_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-2-28b4fd791d9a>:16: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.dense instead.\n",
      "WARNING:tensorflow:From c:\\python36\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From c:\\python36\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "episode: 0   reward: -7\n",
      "episode: 1   reward: -6\n",
      "episode: 2   reward: -5\n",
      "episode: 3   reward: -5\n",
      "episode: 4   reward: -5\n",
      "episode: 5   reward: -5\n",
      "episode: 6   reward: -6\n",
      "episode: 7   reward: -6\n",
      "episode: 8   reward: -6\n",
      "episode: 9   reward: -6\n",
      "episode: 10   reward: -6\n",
      "episode: 11   reward: -6\n",
      "episode: 12   reward: -5\n",
      "episode: 13   reward: -5\n",
      "episode: 14   reward: -5\n",
      "episode: 15   reward: -5\n",
      "episode: 16   reward: -5\n",
      "episode: 17   reward: -5\n",
      "episode: 18   reward: -5\n",
      "episode: 19   reward: -4\n",
      "episode: 20   reward: -4\n",
      "episode: 21   reward: -3\n",
      "episode: 22   reward: -3\n",
      "episode: 23   reward: -3\n",
      "episode: 24   reward: -4\n",
      "episode: 25   reward: -3\n",
      "episode: 26   reward: -3\n",
      "episode: 27   reward: -4\n",
      "episode: 28   reward: -3\n",
      "episode: 29   reward: -3\n",
      "episode: 30   reward: -2\n",
      "episode: 31   reward: -2\n",
      "episode: 32   reward: -2\n",
      "episode: 33   reward: -1\n",
      "episode: 34   reward: -1\n",
      "episode: 35   reward: -2\n",
      "episode: 36   reward: -1\n",
      "episode: 37   reward: -2\n",
      "episode: 38   reward: 0\n",
      "episode: 39   reward: 0\n",
      "episode: 40   reward: 0\n",
      "episode: 41   reward: 0\n",
      "episode: 42   reward: 0\n",
      "episode: 43   reward: 0\n",
      "episode: 44   reward: 0\n",
      "episode: 45   reward: -1\n",
      "episode: 46   reward: -1\n",
      "episode: 47   reward: -1\n",
      "episode: 48   reward: -2\n",
      "episode: 49   reward: -2\n",
      "episode: 50   reward: -2\n",
      "episode: 51   reward: -2\n",
      "episode: 52   reward: -2\n",
      "episode: 53   reward: -1\n",
      "episode: 54   reward: 0\n",
      "episode: 55   reward: 1\n",
      "episode: 56   reward: 3\n",
      "episode: 57   reward: 5\n",
      "episode: 58   reward: 5\n",
      "episode: 59   reward: 7\n",
      "episode: 60   reward: 7\n",
      "episode: 61   reward: 7\n",
      "episode: 62   reward: 7\n",
      "episode: 63   reward: 6\n",
      "episode: 64   reward: 6\n",
      "episode: 65   reward: 7\n",
      "episode: 66   reward: 6\n",
      "episode: 67   reward: 5\n",
      "episode: 68   reward: 5\n",
      "episode: 69   reward: 6\n",
      "episode: 70   reward: 5\n",
      "episode: 71   reward: 5\n",
      "episode: 72   reward: 6\n",
      "episode: 73   reward: 6\n",
      "episode: 74   reward: 6\n",
      "episode: 75   reward: 5\n",
      "episode: 76   reward: 5\n",
      "episode: 77   reward: 5\n",
      "episode: 78   reward: 5\n",
      "episode: 79   reward: 7\n",
      "episode: 80   reward: 10\n",
      "episode: 81   reward: 9\n",
      "episode: 82   reward: 9\n",
      "episode: 83   reward: 9\n",
      "episode: 84   reward: 8\n",
      "episode: 85   reward: 8\n",
      "episode: 86   reward: 8\n",
      "episode: 87   reward: 8\n",
      "episode: 88   reward: 8\n",
      "episode: 89   reward: 8\n",
      "episode: 90   reward: 8\n",
      "episode: 91   reward: 7\n",
      "episode: 92   reward: 9\n",
      "episode: 93   reward: 9\n",
      "episode: 94   reward: 8\n",
      "episode: 95   reward: 8\n",
      "episode: 96   reward: 9\n",
      "episode: 97   reward: 10\n",
      "episode: 98   reward: 9\n",
      "episode: 99   reward: 9\n",
      "episode: 100   reward: 15\n",
      "episode: 101   reward: 15\n",
      "episode: 102   reward: 15\n",
      "episode: 103   reward: 15\n",
      "episode: 104   reward: 16\n",
      "episode: 105   reward: 17\n",
      "episode: 106   reward: 18\n",
      "episode: 107   reward: 17\n",
      "episode: 108   reward: 19\n",
      "episode: 109   reward: 19\n",
      "episode: 110   reward: 23\n",
      "episode: 111   reward: 23\n",
      "episode: 112   reward: 23\n",
      "episode: 113   reward: 24\n",
      "episode: 114   reward: 24\n",
      "episode: 115   reward: 24\n",
      "episode: 116   reward: 24\n",
      "episode: 117   reward: 25\n",
      "episode: 118   reward: 26\n",
      "episode: 119   reward: 27\n",
      "episode: 120   reward: 27\n",
      "episode: 121   reward: 28\n",
      "episode: 122   reward: 30\n",
      "episode: 123   reward: 30\n",
      "episode: 124   reward: 30\n",
      "episode: 125   reward: 29\n",
      "episode: 126   reward: 33\n",
      "episode: 127   reward: 34\n",
      "episode: 128   reward: 37\n",
      "episode: 129   reward: 43\n",
      "episode: 130   reward: 47\n",
      "episode: 131   reward: 51\n",
      "episode: 132   reward: 55\n",
      "episode: 133   reward: 65\n",
      "episode: 134   reward: 93\n",
      "episode: 135   reward: 98\n",
      "episode: 136   reward: 104\n",
      "episode: 137   reward: 104\n",
      "episode: 138   reward: 99\n",
      "episode: 139   reward: 98\n",
      "episode: 140   reward: 95\n",
      "episode: 141   reward: 94\n",
      "episode: 142   reward: 91\n",
      "episode: 143   reward: 92\n",
      "episode: 144   reward: 92\n",
      "episode: 145   reward: 94\n",
      "episode: 146   reward: 97\n",
      "episode: 147   reward: 99\n",
      "episode: 148   reward: 97\n",
      "episode: 149   reward: 95\n",
      "episode: 150   reward: 96\n",
      "episode: 151   reward: 94\n",
      "episode: 152   reward: 95\n",
      "episode: 153   reward: 96\n",
      "episode: 154   reward: 100\n",
      "episode: 155   reward: 105\n",
      "episode: 156   reward: 112\n",
      "episode: 157   reward: 119\n",
      "episode: 158   reward: 130\n",
      "episode: 159   reward: 151\n",
      "episode: 160   reward: 155\n",
      "episode: 161   reward: 152\n",
      "episode: 162   reward: 150\n",
      "episode: 163   reward: 145\n",
      "episode: 164   reward: 144\n",
      "episode: 165   reward: 145\n",
      "episode: 166   reward: 146\n",
      "episode: 167   reward: 160\n",
      "episode: 168   reward: 162\n",
      "episode: 169   reward: 160\n",
      "episode: 170   reward: 159\n",
      "episode: 171   reward: 158\n",
      "episode: 172   reward: 155\n",
      "episode: 173   reward: 154\n",
      "episode: 174   reward: 152\n",
      "episode: 175   reward: 155\n",
      "episode: 176   reward: 156\n",
      "episode: 177   reward: 156\n",
      "episode: 178   reward: 152\n",
      "episode: 179   reward: 150\n",
      "episode: 180   reward: 146\n",
      "episode: 181   reward: 143\n",
      "episode: 182   reward: 141\n",
      "episode: 183   reward: 140\n",
      "episode: 184   reward: 138\n",
      "episode: 185   reward: 138\n",
      "episode: 186   reward: 139\n",
      "episode: 187   reward: 141\n",
      "episode: 188   reward: 144\n",
      "episode: 189   reward: 148\n",
      "episode: 190   reward: 157\n",
      "episode: 191   reward: 164\n",
      "episode: 192   reward: 166\n",
      "episode: 193   reward: 163\n",
      "episode: 194   reward: 162\n",
      "episode: 195   reward: 165\n",
      "episode: 196   reward: 164\n",
      "episode: 197   reward: 164\n",
      "episode: 198   reward: 163\n",
      "episode: 199   reward: 164\n",
      "episode: 200   reward: 165\n",
      "episode: 201   reward: 165\n",
      "episode: 202   reward: 162\n",
      "episode: 203   reward: 160\n",
      "episode: 204   reward: 159\n",
      "episode: 205   reward: 157\n",
      "episode: 206   reward: 154\n",
      "episode: 207   reward: 155\n",
      "episode: 208   reward: 153\n",
      "episode: 209   reward: 152\n",
      "episode: 210   reward: 151\n",
      "episode: 211   reward: 151\n",
      "episode: 212   reward: 160\n",
      "episode: 213   reward: 168\n",
      "episode: 214   reward: 169\n",
      "episode: 215   reward: 168\n",
      "episode: 216   reward: 171\n",
      "episode: 217   reward: 172\n",
      "episode: 218   reward: 170\n",
      "episode: 219   reward: 166\n",
      "episode: 220   reward: 162\n",
      "episode: 221   reward: 158\n",
      "episode: 222   reward: 155\n",
      "episode: 223   reward: 148\n",
      "episode: 224   reward: 142\n",
      "episode: 225   reward: 138\n",
      "episode: 226   reward: 137\n",
      "episode: 227   reward: 134\n",
      "episode: 228   reward: 135\n",
      "episode: 229   reward: 134\n",
      "episode: 230   reward: 133\n",
      "episode: 231   reward: 139\n",
      "episode: 232   reward: 145\n",
      "episode: 233   reward: 147\n",
      "episode: 234   reward: 149\n",
      "episode: 235   reward: 156\n",
      "episode: 236   reward: 159\n",
      "episode: 237   reward: 160\n",
      "episode: 238   reward: 158\n",
      "episode: 239   reward: 159\n",
      "episode: 240   reward: 166\n",
      "episode: 241   reward: 168\n",
      "episode: 242   reward: 167\n",
      "episode: 243   reward: 164\n",
      "episode: 244   reward: 164\n",
      "episode: 245   reward: 162\n",
      "episode: 246   reward: 162\n",
      "episode: 247   reward: 162\n",
      "episode: 248   reward: 164\n",
      "episode: 249   reward: 165\n",
      "episode: 250   reward: 166\n",
      "episode: 251   reward: 164\n",
      "episode: 252   reward: 162\n",
      "episode: 253   reward: 160\n",
      "episode: 254   reward: 160\n",
      "episode: 255   reward: 164\n",
      "episode: 256   reward: 163\n",
      "episode: 257   reward: 163\n",
      "episode: 258   reward: 168\n",
      "episode: 259   reward: 165\n",
      "episode: 260   reward: 162\n",
      "episode: 261   reward: 161\n",
      "episode: 262   reward: 165\n",
      "episode: 263   reward: 166\n",
      "episode: 264   reward: 167\n",
      "episode: 265   reward: 178\n",
      "episode: 266   reward: 187\n",
      "episode: 267   reward: 188\n",
      "episode: 268   reward: 183\n",
      "episode: 269   reward: 175\n",
      "episode: 270   reward: 167\n",
      "episode: 271   reward: 160\n",
      "episode: 272   reward: 156\n",
      "episode: 273   reward: 157\n",
      "episode: 274   reward: 188\n",
      "episode: 275   reward: 196\n",
      "episode: 276   reward: 223\n",
      "episode: 277   reward: 226\n",
      "episode: 278   reward: 221\n",
      "episode: 279   reward: 218\n",
      "episode: 280   reward: 219\n",
      "episode: 281   reward: 214\n",
      "episode: 282   reward: 211\n",
      "episode: 283   reward: 206\n",
      "episode: 284   reward: 200\n",
      "episode: 285   reward: 195\n",
      "episode: 286   reward: 192\n",
      "episode: 287   reward: 188\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 288   reward: 183\n",
      "episode: 289   reward: 179\n",
      "episode: 290   reward: 177\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-6887241131a0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     14\u001b[0m     \u001b[0mtrack_r\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m     \u001b[1;32mwhile\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 16\u001b[1;33m         \u001b[1;32mif\u001b[0m \u001b[0mRENDER\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     17\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m         \u001b[0ma\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mactor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mchoose_action\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\python36\\lib\\site-packages\\gym\\envs\\classic_control\\cartpole.py\u001b[0m in \u001b[0;36mrender\u001b[1;34m(self, mode)\u001b[0m\n\u001b[0;32m    186\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpoletrans\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_rotation\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    187\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 188\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mviewer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mreturn_rgb_array\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m==\u001b[0m\u001b[1;34m'rgb_array'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    189\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    190\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\python36\\lib\\site-packages\\gym\\envs\\classic_control\\rendering.py\u001b[0m in \u001b[0;36mrender\u001b[1;34m(self, return_rgb_array)\u001b[0m\n\u001b[0;32m    112\u001b[0m             \u001b[0marr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0marr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbuffer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mheight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwidth\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m4\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    113\u001b[0m             \u001b[0marr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0marr\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 114\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwindow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    115\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0monetime_geoms\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    116\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0marr\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mreturn_rgb_array\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0misopen\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\python36\\lib\\site-packages\\pyglet\\window\\win32\\__init__.py\u001b[0m in \u001b[0;36mflip\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    319\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mflip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    320\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdraw_mouse_cursor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 321\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    322\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    323\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mset_location\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\python36\\lib\\site-packages\\pyglet\\gl\\win32.py\u001b[0m in \u001b[0;36mflip\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    224\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    225\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mflip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 226\u001b[1;33m         \u001b[0m_gdi32\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSwapBuffers\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcanvas\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhdc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    227\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    228\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mget_vsync\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "sess = tf.Session()\n",
    "\n",
    "actor = Actor(sess, n_features=N_F, n_actions=N_A, lr=LR_A)\n",
    "critic = Critic(sess, n_features=N_F, lr=LR_C)     # we need a good teacher, so the teacher should learn faster than the actor\n",
    "\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "if OUTPUT_GRAPH:\n",
    "    tf.summary.FileWriter(\"logs/\", sess.graph)\n",
    "\n",
    "for i_episode in range(MAX_EPISODE):\n",
    "    s = env.reset()\n",
    "    t = 0\n",
    "    track_r = []\n",
    "    while True:\n",
    "        if RENDER: env.render()\n",
    "\n",
    "        a = actor.choose_action(s)\n",
    "\n",
    "        s_, r, done, info = env.step(a)\n",
    "\n",
    "        if done: r = -20\n",
    "\n",
    "        track_r.append(r)\n",
    "\n",
    "        td_error = critic.learn(s, r, s_)  # gradient = grad[r + gamma * V(s_) - V(s)]\n",
    "        actor.learn(s, a, td_error)     # true_gradient = grad[logPi(s,a) * td_error]\n",
    "\n",
    "        s = s_\n",
    "        t += 1\n",
    "\n",
    "        if done or t >= MAX_EP_STEPS:\n",
    "            ep_rs_sum = sum(track_r)\n",
    "\n",
    "            if 'running_reward' not in globals():\n",
    "                running_reward = ep_rs_sum\n",
    "            else:\n",
    "                running_reward = running_reward * 0.95 + ep_rs_sum * 0.05\n",
    "            if running_reward > DISPLAY_REWARD_THRESHOLD: RENDER = True  # rendering\n",
    "            print(\"episode:\", i_episode, \"  reward:\", int(running_reward))\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
