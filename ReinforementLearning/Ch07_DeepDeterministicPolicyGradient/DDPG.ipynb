{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import gym\n",
    "import time\n",
    "\n",
    "\n",
    "np.random.seed(1)\n",
    "tf.set_random_seed(1)\n",
    "\n",
    "#####################  hyper parameters  ####################\n",
    "\n",
    "MAX_EPISODES = 200\n",
    "MAX_EP_STEPS = 200\n",
    "LR_A = 0.001    # learning rate for actor\n",
    "LR_C = 0.001    # learning rate for critic\n",
    "GAMMA = 0.9     # reward discount\n",
    "REPLACEMENT = [\n",
    "    dict(name='soft', tau=0.01),\n",
    "    dict(name='hard', rep_iter_a=600, rep_iter_c=500)\n",
    "][0]            # you can try different target replacement strategies\n",
    "MEMORY_CAPACITY = 10000\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "RENDER = False\n",
    "OUTPUT_GRAPH = True\n",
    "ENV_NAME = 'Pendulum-v0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################  Actor  ####################################\n",
    "\n",
    "\n",
    "class Actor(object):\n",
    "    def __init__(self, sess, action_dim, action_bound, learning_rate, replacement):\n",
    "        self.sess = sess\n",
    "        self.a_dim = action_dim\n",
    "        self.action_bound = action_bound\n",
    "        self.lr = learning_rate\n",
    "        self.replacement = replacement\n",
    "        self.t_replace_counter = 0\n",
    "\n",
    "        with tf.variable_scope('Actor'):\n",
    "            # input s, output a\n",
    "            self.a = self._build_net(S, scope='eval_net', trainable=True)\n",
    "\n",
    "            # input s_, output a, get a_ for critic\n",
    "            self.a_ = self._build_net(S_, scope='target_net', trainable=False)\n",
    "\n",
    "        self.e_params = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope='Actor/eval_net')\n",
    "        self.t_params = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope='Actor/target_net')\n",
    "\n",
    "        if self.replacement['name'] == 'hard':\n",
    "            self.t_replace_counter = 0\n",
    "            self.hard_replace = [tf.assign(t, e) for t, e in zip(self.t_params, self.e_params)]\n",
    "        else:\n",
    "            self.soft_replace = [tf.assign(t, (1 - self.replacement['tau']) * t + self.replacement['tau'] * e)\n",
    "                                 for t, e in zip(self.t_params, self.e_params)]\n",
    "\n",
    "    def _build_net(self, s, scope, trainable):\n",
    "        with tf.variable_scope(scope):\n",
    "            init_w = tf.random_normal_initializer(0., 0.3)\n",
    "            init_b = tf.constant_initializer(0.1)\n",
    "            net = tf.layers.dense(s, 30, activation=tf.nn.relu,\n",
    "                                  kernel_initializer=init_w, bias_initializer=init_b, name='l1',\n",
    "                                  trainable=trainable)\n",
    "            with tf.variable_scope('a'):\n",
    "                actions = tf.layers.dense(net, self.a_dim, activation=tf.nn.tanh, kernel_initializer=init_w,\n",
    "                                          bias_initializer=init_b, name='a', trainable=trainable)\n",
    "                scaled_a = tf.multiply(actions, self.action_bound, name='scaled_a')  # Scale output to -action_bound to action_bound\n",
    "        return scaled_a\n",
    "\n",
    "    def learn(self, s):   # batch update\n",
    "        self.sess.run(self.train_op, feed_dict={S: s})\n",
    "\n",
    "        if self.replacement['name'] == 'soft':\n",
    "            self.sess.run(self.soft_replace)\n",
    "        else:\n",
    "            if self.t_replace_counter % self.replacement['rep_iter_a'] == 0:\n",
    "                self.sess.run(self.hard_replace)\n",
    "            self.t_replace_counter += 1\n",
    "\n",
    "    def choose_action(self, s):\n",
    "        s = s[np.newaxis, :]    # single state\n",
    "        return self.sess.run(self.a, feed_dict={S: s})[0]  # single action\n",
    "\n",
    "    def add_grad_to_graph(self, a_grads):\n",
    "        with tf.variable_scope('policy_grads'):\n",
    "            # ys = policy;\n",
    "            # xs = policy's parameters;\n",
    "            # a_grads = the gradients of the policy to get more Q\n",
    "            # tf.gradients will calculate dys/dxs with a initial gradients for ys, so this is dq/da * da/dparams\n",
    "            self.policy_grads = tf.gradients(ys=self.a, xs=self.e_params, grad_ys=a_grads)\n",
    "\n",
    "        with tf.variable_scope('A_train'):\n",
    "            opt = tf.train.AdamOptimizer(-self.lr)  # (- learning rate) for ascent policy\n",
    "            self.train_op = opt.apply_gradients(zip(self.policy_grads, self.e_params))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################  Critic  ####################################\n",
    "\n",
    "class Critic(object):\n",
    "    def __init__(self, sess, state_dim, action_dim, learning_rate, gamma, replacement, a, a_):\n",
    "        self.sess = sess\n",
    "        self.s_dim = state_dim\n",
    "        self.a_dim = action_dim\n",
    "        self.lr = learning_rate\n",
    "        self.gamma = gamma\n",
    "        self.replacement = replacement\n",
    "\n",
    "        with tf.variable_scope('Critic'):\n",
    "            # Input (s, a), output q\n",
    "            self.a = tf.stop_gradient(a)    # stop critic update flows to actor\n",
    "            self.q = self._build_net(S, self.a, 'eval_net', trainable=True)\n",
    "\n",
    "            # Input (s_, a_), output q_ for q_target\n",
    "            self.q_ = self._build_net(S_, a_, 'target_net', trainable=False)    # target_q is based on a_ from Actor's target_net\n",
    "\n",
    "            self.e_params = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope='Critic/eval_net')\n",
    "            self.t_params = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope='Critic/target_net')\n",
    "\n",
    "        with tf.variable_scope('target_q'):\n",
    "            self.target_q = R + self.gamma * self.q_\n",
    "\n",
    "        with tf.variable_scope('TD_error'):\n",
    "            self.loss = tf.reduce_mean(tf.squared_difference(self.target_q, self.q))\n",
    "\n",
    "        with tf.variable_scope('C_train'):\n",
    "            self.train_op = tf.train.AdamOptimizer(self.lr).minimize(self.loss)\n",
    "\n",
    "        with tf.variable_scope('a_grad'):\n",
    "            self.a_grads = tf.gradients(self.q, a)[0]   # tensor of gradients of each sample (None, a_dim)\n",
    "\n",
    "        if self.replacement['name'] == 'hard':\n",
    "            self.t_replace_counter = 0\n",
    "            self.hard_replacement = [tf.assign(t, e) for t, e in zip(self.t_params, self.e_params)]\n",
    "        else:\n",
    "            self.soft_replacement = [tf.assign(t, (1 - self.replacement['tau']) * t + self.replacement['tau'] * e)\n",
    "                                     for t, e in zip(self.t_params, self.e_params)]\n",
    "\n",
    "    def _build_net(self, s, a, scope, trainable):\n",
    "        with tf.variable_scope(scope):\n",
    "            init_w = tf.random_normal_initializer(0., 0.1)\n",
    "            init_b = tf.constant_initializer(0.1)\n",
    "\n",
    "            with tf.variable_scope('l1'):\n",
    "                n_l1 = 30\n",
    "                w1_s = tf.get_variable('w1_s', [self.s_dim, n_l1], initializer=init_w, trainable=trainable)\n",
    "                w1_a = tf.get_variable('w1_a', [self.a_dim, n_l1], initializer=init_w, trainable=trainable)\n",
    "                b1 = tf.get_variable('b1', [1, n_l1], initializer=init_b, trainable=trainable)\n",
    "                net = tf.nn.relu(tf.matmul(s, w1_s) + tf.matmul(a, w1_a) + b1)\n",
    "\n",
    "            with tf.variable_scope('q'):\n",
    "                q = tf.layers.dense(net, 1, kernel_initializer=init_w, bias_initializer=init_b, trainable=trainable)   # Q(s,a)\n",
    "        return q\n",
    "\n",
    "    def learn(self, s, a, r, s_):\n",
    "        self.sess.run(self.train_op, feed_dict={S: s, self.a: a, R: r, S_: s_})\n",
    "        if self.replacement['name'] == 'soft':\n",
    "            self.sess.run(self.soft_replacement)\n",
    "        else:\n",
    "            if self.t_replace_counter % self.replacement['rep_iter_c'] == 0:\n",
    "                self.sess.run(self.hard_replacement)\n",
    "            self.t_replace_counter += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#####################  Memory  ####################\n",
    "\n",
    "class Memory(object):\n",
    "    def __init__(self, capacity, dims):\n",
    "        self.capacity = capacity\n",
    "        self.data = np.zeros((capacity, dims))\n",
    "        self.pointer = 0\n",
    "\n",
    "    def store_transition(self, s, a, r, s_):\n",
    "        transition = np.hstack((s, a, [r], s_))\n",
    "        index = self.pointer % self.capacity  # replace the old memory with new memory\n",
    "        self.data[index, :] = transition\n",
    "        self.pointer += 1\n",
    "\n",
    "    def sample(self, n):\n",
    "        assert self.pointer >= self.capacity, 'Memory has not been fulfilled'\n",
    "        indices = np.random.choice(self.capacity, size=n)\n",
    "        return self.data[indices, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-2-6c306a090202>:36: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.dense instead.\n",
      "WARNING:tensorflow:From c:\\python36\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "Episode: 0  Reward: -1271 Explore: 3.00\n",
      "Episode: 1  Reward: -1300 Explore: 3.00\n",
      "Episode: 2  Reward: -1238 Explore: 3.00\n",
      "Episode: 3  Reward: -1480 Explore: 3.00\n",
      "Episode: 4  Reward: -1423 Explore: 3.00\n",
      "Episode: 5  Reward: -1664 Explore: 3.00\n",
      "Episode: 6  Reward: -1115 Explore: 3.00\n",
      "Episode: 7  Reward: -1179 Explore: 3.00\n",
      "Episode: 8  Reward: -1168 Explore: 3.00\n",
      "Episode: 9  Reward: -1043 Explore: 3.00\n",
      "Episode: 10  Reward: -1261 Explore: 3.00\n",
      "Episode: 11  Reward: -1133 Explore: 3.00\n",
      "Episode: 12  Reward: -1810 Explore: 3.00\n",
      "Episode: 13  Reward: -1196 Explore: 3.00\n",
      "Episode: 14  Reward: -1274 Explore: 3.00\n",
      "Episode: 15  Reward: -1518 Explore: 3.00\n",
      "Episode: 16  Reward: -1331 Explore: 3.00\n",
      "Episode: 17  Reward: -1247 Explore: 3.00\n",
      "Episode: 18  Reward: -1338 Explore: 3.00\n",
      "Episode: 19  Reward: -1413 Explore: 3.00\n",
      "Episode: 20  Reward: -1605 Explore: 3.00\n",
      "Episode: 21  Reward: -1197 Explore: 3.00\n",
      "Episode: 22  Reward: -1340 Explore: 3.00\n",
      "Episode: 23  Reward: -1686 Explore: 3.00\n",
      "Episode: 24  Reward: -1367 Explore: 3.00\n",
      "Episode: 25  Reward: -1251 Explore: 3.00\n",
      "Episode: 26  Reward: -1128 Explore: 3.00\n",
      "Episode: 27  Reward: -1170 Explore: 3.00\n",
      "Episode: 28  Reward: -1274 Explore: 3.00\n",
      "Episode: 29  Reward: -1129 Explore: 3.00\n",
      "Episode: 30  Reward: -1269 Explore: 3.00\n",
      "Episode: 31  Reward: -1189 Explore: 3.00\n",
      "Episode: 32  Reward: -1325 Explore: 3.00\n",
      "Episode: 33  Reward: -1338 Explore: 3.00\n",
      "Episode: 34  Reward: -1199 Explore: 3.00\n",
      "Episode: 35  Reward: -1311 Explore: 3.00\n",
      "Episode: 36  Reward: -1202 Explore: 3.00\n",
      "Episode: 37  Reward: -1137 Explore: 3.00\n",
      "Episode: 38  Reward: -1092 Explore: 3.00\n",
      "Episode: 39  Reward: -1177 Explore: 3.00\n",
      "Episode: 40  Reward: -1298 Explore: 3.00\n",
      "Episode: 41  Reward: -1250 Explore: 3.00\n",
      "Episode: 42  Reward: -1056 Explore: 3.00\n",
      "Episode: 43  Reward: -1178 Explore: 3.00\n",
      "Episode: 44  Reward: -1235 Explore: 3.00\n",
      "Episode: 45  Reward: -1513 Explore: 3.00\n",
      "Episode: 46  Reward: -1344 Explore: 3.00\n",
      "Episode: 47  Reward: -1633 Explore: 3.00\n",
      "Episode: 48  Reward: -1289 Explore: 3.00\n",
      "Episode: 49  Reward: -1595 Explore: 3.00\n",
      "Episode: 50  Reward: -1312 Explore: 2.71\n",
      "Episode: 51  Reward: -1295 Explore: 2.46\n",
      "Episode: 52  Reward: -1325 Explore: 2.22\n",
      "Episode: 53  Reward: -1298 Explore: 2.01\n",
      "Episode: 54  Reward: -1608 Explore: 1.82\n",
      "Episode: 55  Reward: -1111 Explore: 1.65\n",
      "Episode: 56  Reward: -1309 Explore: 1.49\n",
      "Episode: 57  Reward: -1516 Explore: 1.35\n",
      "Episode: 58  Reward: -1505 Explore: 1.22\n",
      "Episode: 59  Reward: -1132 Explore: 1.10\n",
      "Episode: 60  Reward: -1265 Explore: 1.00\n",
      "Episode: 61  Reward: -1320 Explore: 0.90\n",
      "Episode: 62  Reward: -1470 Explore: 0.82\n",
      "Episode: 63  Reward: -1545 Explore: 0.74\n",
      "Episode: 64  Reward: -1251 Explore: 0.67\n",
      "Episode: 65  Reward: -1375 Explore: 0.61\n",
      "Episode: 66  Reward: -1169 Explore: 0.55\n",
      "Episode: 67  Reward: -1549 Explore: 0.50\n",
      "Episode: 68  Reward: -1554 Explore: 0.45\n",
      "Episode: 69  Reward: -1535 Explore: 0.41\n",
      "Episode: 70  Reward: -1633 Explore: 0.37\n",
      "Episode: 71  Reward: -1547 Explore: 0.33\n",
      "Episode: 72  Reward: -1287 Explore: 0.30\n",
      "Episode: 73  Reward: -1161 Explore: 0.27\n",
      "Episode: 74  Reward: -1628 Explore: 0.25\n",
      "Episode: 75  Reward: -1095 Explore: 0.22\n",
      "Episode: 76  Reward: -1382 Explore: 0.20\n",
      "Episode: 77  Reward: -1647 Explore: 0.18\n",
      "Episode: 78  Reward: -1514 Explore: 0.16\n",
      "Episode: 79  Reward: -1211 Explore: 0.15\n",
      "Episode: 80  Reward: -1513 Explore: 0.14\n",
      "Episode: 81  Reward: -1543 Explore: 0.12\n",
      "Episode: 82  Reward: -1501 Explore: 0.11\n",
      "Episode: 83  Reward: -1459 Explore: 0.10\n",
      "Episode: 84  Reward: -945 Explore: 0.09\n",
      "Episode: 85  Reward: -1616 Explore: 0.08\n",
      "Episode: 86  Reward: -1642 Explore: 0.07\n",
      "Episode: 87  Reward: -1639 Explore: 0.07\n",
      "Episode: 88  Reward: -1647 Explore: 0.06\n",
      "Episode: 89  Reward: -1189 Explore: 0.05\n",
      "Episode: 90  Reward: -1499 Explore: 0.05\n",
      "Episode: 91  Reward: -1158 Explore: 0.04\n",
      "Episode: 92  Reward: -1332 Explore: 0.04\n",
      "Episode: 93  Reward: -1654 Explore: 0.04\n",
      "Episode: 94  Reward: -1549 Explore: 0.03\n",
      "Episode: 95  Reward: -1339 Explore: 0.03\n",
      "Episode: 96  Reward: -1646 Explore: 0.03\n",
      "Episode: 97  Reward: -1058 Explore: 0.02\n",
      "Episode: 98  Reward: -721 Explore: 0.02\n",
      "Episode: 99  Reward: -1474 Explore: 0.02\n",
      "Episode: 100  Reward: -1525 Explore: 0.02\n",
      "Episode: 101  Reward: -1631 Explore: 0.02\n",
      "Episode: 102  Reward: -1494 Explore: 0.01\n",
      "Episode: 103  Reward: -1607 Explore: 0.01\n",
      "Episode: 104  Reward: -1608 Explore: 0.01\n",
      "Episode: 105  Reward: -1562 Explore: 0.01\n",
      "Episode: 106  Reward: -1520 Explore: 0.01\n",
      "Episode: 107  Reward: -1119 Explore: 0.01\n",
      "Episode: 108  Reward: -1365 Explore: 0.01\n",
      "Episode: 109  Reward: -1136 Explore: 0.01\n",
      "Episode: 110  Reward: -937 Explore: 0.01\n",
      "Episode: 111  Reward: -1278 Explore: 0.01\n",
      "Episode: 112  Reward: -1502 Explore: 0.01\n",
      "Episode: 113  Reward: -1616 Explore: 0.00\n",
      "Episode: 114  Reward: -1537 Explore: 0.00\n",
      "Episode: 115  Reward: -1476 Explore: 0.00\n",
      "Episode: 116  Reward: -1377 Explore: 0.00\n",
      "Episode: 117  Reward: -1502 Explore: 0.00\n",
      "Episode: 118  Reward: -1633 Explore: 0.00\n",
      "Episode: 119  Reward: -1315 Explore: 0.00\n",
      "Episode: 120  Reward: -1634 Explore: 0.00\n",
      "Episode: 121  Reward: -1302 Explore: 0.00\n",
      "Episode: 122  Reward: -1278 Explore: 0.00\n",
      "Episode: 123  Reward: -1621 Explore: 0.00\n",
      "Episode: 124  Reward: -1108 Explore: 0.00\n",
      "Episode: 125  Reward: -1207 Explore: 0.00\n",
      "Episode: 126  Reward: -1641 Explore: 0.00\n",
      "Episode: 127  Reward: -838 Explore: 0.00\n",
      "Episode: 128  Reward: -1551 Explore: 0.00\n",
      "Episode: 129  Reward: -1568 Explore: 0.00\n",
      "Episode: 130  Reward: -1540 Explore: 0.00\n",
      "Episode: 131  Reward: -1510 Explore: 0.00\n",
      "Episode: 132  Reward: -1292 Explore: 0.00\n",
      "Episode: 133  Reward: -1648 Explore: 0.00\n",
      "Episode: 134  Reward: -1490 Explore: 0.00\n",
      "Episode: 135  Reward: -1654 Explore: 0.00\n",
      "Episode: 136  Reward: -1503 Explore: 0.00\n",
      "Episode: 137  Reward: -1397 Explore: 0.00\n",
      "Episode: 138  Reward: -1345 Explore: 0.00\n",
      "Episode: 139  Reward: -1283 Explore: 0.00\n",
      "Episode: 140  Reward: -1286 Explore: 0.00\n",
      "Episode: 141  Reward: -1483 Explore: 0.00\n",
      "Episode: 142  Reward: -1651 Explore: 0.00\n",
      "Episode: 143  Reward: -954 Explore: 0.00\n",
      "Episode: 144  Reward: -1653 Explore: 0.00\n",
      "Episode: 145  Reward: -1010 Explore: 0.00\n",
      "Episode: 146  Reward: -1491 Explore: 0.00\n",
      "Episode: 147  Reward: -1360 Explore: 0.00\n",
      "Episode: 148  Reward: -1494 Explore: 0.00\n",
      "Episode: 149  Reward: -1196 Explore: 0.00\n",
      "Episode: 150  Reward: -1593 Explore: 0.00\n",
      "Episode: 151  Reward: -1494 Explore: 0.00\n",
      "Episode: 152  Reward: -1506 Explore: 0.00\n",
      "Episode: 153  Reward: -1569 Explore: 0.00\n",
      "Episode: 154  Reward: -1407 Explore: 0.00\n",
      "Episode: 155  Reward: -1133 Explore: 0.00\n",
      "Episode: 156  Reward: -1502 Explore: 0.00\n",
      "Episode: 157  Reward: -1330 Explore: 0.00\n",
      "Episode: 158  Reward: -1654 Explore: 0.00\n",
      "Episode: 159  Reward: -1438 Explore: 0.00\n",
      "Episode: 160  Reward: -1642 Explore: 0.00\n",
      "Episode: 161  Reward: -1504 Explore: 0.00\n",
      "Episode: 162  Reward: -1657 Explore: 0.00\n",
      "Episode: 163  Reward: -1637 Explore: 0.00\n",
      "Episode: 164  Reward: -1243 Explore: 0.00\n",
      "Episode: 165  Reward: -1053 Explore: 0.00\n",
      "Episode: 166  Reward: -1511 Explore: 0.00\n",
      "Episode: 167  Reward: -1623 Explore: 0.00\n",
      "Episode: 168  Reward: -1067 Explore: 0.00\n",
      "Episode: 169  Reward: -946 Explore: 0.00\n",
      "Episode: 170  Reward: -1433 Explore: 0.00\n",
      "Episode: 171  Reward: -1495 Explore: 0.00\n",
      "Episode: 172  Reward: -1656 Explore: 0.00\n",
      "Episode: 173  Reward: -1651 Explore: 0.00\n",
      "Episode: 174  Reward: -1656 Explore: 0.00\n",
      "Episode: 175  Reward: -1036 Explore: 0.00\n",
      "Episode: 176  Reward: -1629 Explore: 0.00\n",
      "Episode: 177  Reward: -1219 Explore: 0.00\n",
      "Episode: 178  Reward: -1218 Explore: 0.00\n",
      "Episode: 179  Reward: -1213 Explore: 0.00\n",
      "Episode: 180  Reward: -1652 Explore: 0.00\n",
      "Episode: 181  Reward: -1516 Explore: 0.00\n",
      "Episode: 182  Reward: -922 Explore: 0.00\n",
      "Episode: 183  Reward: -947 Explore: 0.00\n",
      "Episode: 184  Reward: -1389 Explore: 0.00\n",
      "Episode: 185  Reward: -1346 Explore: 0.00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 186  Reward: -1112 Explore: 0.00\n",
      "Episode: 187  Reward: -1650 Explore: 0.00\n",
      "Episode: 188  Reward: -939 Explore: 0.00\n",
      "Episode: 189  Reward: -1638 Explore: 0.00\n",
      "Episode: 190  Reward: -1274 Explore: 0.00\n",
      "Episode: 191  Reward: -1495 Explore: 0.00\n",
      "Episode: 192  Reward: -1645 Explore: 0.00\n",
      "Episode: 193  Reward: -1533 Explore: 0.00\n",
      "Episode: 194  Reward: -1491 Explore: 0.00\n",
      "Episode: 195  Reward: -1533 Explore: 0.00\n",
      "Episode: 196  Reward: -1420 Explore: 0.00\n",
      "Episode: 197  Reward: -1625 Explore: 0.00\n",
      "Episode: 198  Reward: -1409 Explore: 0.00\n",
      "Episode: 199  Reward: -1480 Explore: 0.00\n",
      "Running time:  49.197572231292725\n"
     ]
    }
   ],
   "source": [
    "env = gym.make(ENV_NAME)\n",
    "env = env.unwrapped\n",
    "env.seed(1)\n",
    "\n",
    "state_dim = env.observation_space.shape[0]\n",
    "action_dim = env.action_space.shape[0]\n",
    "action_bound = env.action_space.high\n",
    "\n",
    "# all placeholder for tf\n",
    "with tf.name_scope('S'):\n",
    "    S = tf.placeholder(tf.float32, shape=[None, state_dim], name='s')\n",
    "with tf.name_scope('R'):\n",
    "    R = tf.placeholder(tf.float32, [None, 1], name='r')\n",
    "with tf.name_scope('S_'):\n",
    "    S_ = tf.placeholder(tf.float32, shape=[None, state_dim], name='s_')\n",
    "\n",
    "\n",
    "sess = tf.Session()\n",
    "\n",
    "# Create actor and critic.\n",
    "# They are actually connected to each other, details can be seen in tensorboard or in this picture:\n",
    "actor = Actor(sess, action_dim, action_bound, LR_A, REPLACEMENT)\n",
    "critic = Critic(sess, state_dim, action_dim, LR_C, GAMMA, REPLACEMENT, actor.a, actor.a_)\n",
    "actor.add_grad_to_graph(critic.a_grads)\n",
    "\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "M = Memory(MEMORY_CAPACITY, dims=2 * state_dim + action_dim + 1)\n",
    "\n",
    "if OUTPUT_GRAPH:\n",
    "    tf.summary.FileWriter(\"logs/\", sess.graph)\n",
    "\n",
    "var = 3  # control exploration\n",
    "\n",
    "t1 = time.time()\n",
    "for i in range(MAX_EPISODES):\n",
    "    s = env.reset()\n",
    "    ep_reward = 0\n",
    "\n",
    "    for j in range(MAX_EP_STEPS):\n",
    "\n",
    "        if RENDER:\n",
    "            env.render()\n",
    "\n",
    "        # Add exploration noise\n",
    "        a = actor.choose_action(s)\n",
    "        a = np.clip(np.random.normal(a, var), -2, 2)    # add randomness to action selection for exploration\n",
    "        s_, r, done, info = env.step(a)\n",
    "\n",
    "        M.store_transition(s, a, r / 10, s_)\n",
    "\n",
    "        if M.pointer > MEMORY_CAPACITY:\n",
    "            var *= .9995    # decay the action randomness\n",
    "            b_M = M.sample(BATCH_SIZE)\n",
    "            b_s = b_M[:, :state_dim]\n",
    "            b_a = b_M[:, state_dim: state_dim + action_dim]\n",
    "            b_r = b_M[:, -state_dim - 1: -state_dim]\n",
    "            b_s_ = b_M[:, -state_dim:]\n",
    "\n",
    "            critic.learn(b_s, b_a, b_r, b_s_)\n",
    "            actor.learn(b_s)\n",
    "\n",
    "        s = s_\n",
    "        ep_reward += r\n",
    "\n",
    "        if j == MAX_EP_STEPS-1:\n",
    "            print('Episode:', i, ' Reward: %i' % int(ep_reward), 'Explore: %.2f' % var, )\n",
    "            if ep_reward > -300:\n",
    "                RENDER = True\n",
    "            break\n",
    "\n",
    "print('Running time: ', time.time()-t1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
